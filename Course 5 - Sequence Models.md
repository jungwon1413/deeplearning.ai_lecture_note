## Course 5: Sequence Models
### 1주차 (Week 1)
#### Recurrent Neural Networks
- Video: Why Sequence Models
	- Examples of sequence data
		- Speech recognition
			- Given input audio clip X, asked to map it to transcript Y
			- Both the input and the output are the sequence data
		- Music generation
			- No input given, asked to generate music Y
			- Only the output is sequence data
			- Input can be empty set, or a single integer (referring to the genre of the music, etc.)
		- Sentiment classification
			- Ex) "There is nothing to like in this movie" ★☆☆☆☆
			- Input X is sequence, predict how many stars this review would be (Y)
		- DNA sequence analysis
			- Given a DNA sequence, corresponds to which part is the protein
		- Machine translation
			- Ex) "Voulez-vous chanter avec moi?"(French) → "Do you want to sing with me?"(English)
		- Video activity recognition
			- Given a sequence of a video frame, asked to recognize an activity
		- Name entity recognition
			- Ex) "Yesterday, Harry Potter met Hermione Granger," → "Harry Potter", "Hermione Granger"
			- Figure out the names in the sentence
- Video: Notation
	- Motivating Example
		- x: Harry Potter and Hermione Granger invented a new spell.
		<br>[x<1>, x<2>, x<3>, ..., x<9>] \(word sequence)
		<br>Tx = 9
		- y: 1 1 0 1 1 0 0 0 0
		<br>[y<1>, y<2>, y<3>, ..., y<9>]
		<br>Ty = 9
		- x(i)<\t>: i-th example of the input sequence, t-th element
		- Tx(i): input sequence length for training example
		- y(i)<\t>: i-th example of the output sequence, t-th element
		- Ty(i): output sequence length for training example
		- Tx and Ty can be different
		- NLP: Natural Language Processing
	- Representing words
		- Use vocabulary pool: Dictionary
			- In case of the word not on the list, there's <UNK> which is 'unknown word'
		- Dictionary sizes for modern applications with 30,000 ~ 50,000 are more common, and 100,000 is not uncommon.
		- Some of the larger companies uses about 1 million words
		- Commercial applications use 30,000 or maybe 50,000 words
		- Using one-hot representations
			- x<1> = [0 0 0 0 0 0 ... 0 1 0 0 0 0 ...] \(4075th word is 1)
			<br>x<2> = [0 0 0 0 0 0 ... 0 0 0 1 0 ...] \(6830th word is 1)
			<br>x<3> = [0 0 0 0 0 0 ... 1 0 0 0 0 0 ...] \(367th word is 1)
			<br>...
			- This list has length of 10,000 (size of the words)
			- Each of the x<\t> would be one-hot sector
- Video: Recurrent Neural Network Model
	- Why not a standard network?
		- Inputs, outputs can be different lengths in different examples.
		- Doesn't share features learned across different positions of text.
	- Recurrent Neural Networks
		- a<0>: Vector of zeros
		- Tx = Ty case
		- Sometimes denoted as a single neuron picture by recursive arrow
			- To avoid confusion, this lecture will use consecutive draw for recurrent neural networks
		- X to hidden layer: use 'Wax'
		- Horizontal connection between hidden layers: use 'Waa'
		- Output predictions from hidden layer: 'Wya'
		- Hard problems:
			- He said, "Teddy Roosevelt was a great President."
			- He said, "Teddy bears are on sale!"
			- Both sentences include 'He said, "Teddy '
			- Difference between 1st Teddy, and 2nd Teddy?
				- To reduce this problem, Bidirectional RNN is there! (BRNN)
		- Functions
			- a<0> = 0(→)
			- a<1> = g(Waa·a<0> + Wax·x<1> + ba)
				- Tanh/ReLU
			- y_hat<1> = g(Wya·a<1> + by)
				- 'W_ax' details
					- x: it's going to be multiplied by 'x' entities
					- a: this is used to compute 'a'-like quantity
				- Sigmoid
			- a<\t> = g(Waa·a<\t-1> + Wax·x<\t> + ba)
			- y_hat<\t> = g(Wya·a<\t> + by)
	- Simplified RNN notation
		- a<\t> = g(Waa·a<\t-1> + Wax·x<\t> + ba)
			- a<\t> = g(Wa[a<\t-1>, x<\t>] + ba)
			- [Waa|Wax] = Wa (Concatenated)
			- For example,
				- Waa: (100, 100)
				- a<\t-1>: 100
				- Wax(100, 10000)
				- x<\t>: 10000
				- Then, [Waa|Wax]: (100, 100 + 10,000) = Wa
				- [a<\t-1>, x<\t>]
				<br>= [a<\t-1> (height 100)
				<br>x<t>] (height 10,000)
				<br>(height 10,100)
		- y_hat<\t> = g(Wya·a<\t> + by)
- Video: Backpropagation through time
	- Forward propagation and backpropagation
		- L<\t>(y_hat<\t>, y<\t>) = -y<\t>log(y_hat<\t>) - (1 - y<\t>)log(1 - y_hat<\t>)
		- L(y_hat, y) = ∑[1\∽m]L<\t>(y_hat<\t>, y<\t>)
		- "Backpropagation through time"
- Video: Different types of RNNs
	- Examples of sequence data
		- (Examples from first lecture)
	- Examples of RNN architectures
		- Many-to-one
			- Sentiment classification
				- x: text
				- y: 0/1, 0~5, etc.
		- Many-to-many	(input at layer *l* corresponds to output at layer *l*)
			- Tx = Ty
		- One-to-one
		- One-to-many
			- Music generation
				- x: None
				- y: [y<1>, y<2>, y<3>, ...]
		- Many-to-many
			- Machine translation
				- Encoder/decoder
				- Tx ≠ Ty (Seldom equals)
- Video: Language model and sequence generation
	- What is language modelling?
		- Speech recognition
			- Example (sentences with same pronunciations)
				- 1) The apple and pair salad.
				- 2) The apple and pear salad.
				- P(The apple and pair salad) = 3.2 x 10^-13
				- P(The apple and pear salad) = 5.7 x 10^-10
				- P(sentence) = ?
				<br>P(y<1>, y<2>, y<3>, ..., y<Ty>)
	- Language Modelling with an RNN
		- Training set: large corpus of english text.
			- <b>Tokenize</b>
				- Ex) Cat average 15 hours of sleep a day. <EOS>
				<br> y<1> y<2> y<3> ... y<8> y<9>
				- Ex2) The Egyptian Mau is a bread of cat. <EOS>
				<br> Mau is unknown word, therefore it's '<UNK>'
			- x<\t> = y<\t-1>
	- RNN model
		- (Check the slide)
		- L(y_hat<\t>, y<\t>) = -∑[i] yi<\t>log(y_hat(i)<\t>)
		- L = ∑ L<\t>(y_hat<\t>, y<\t>)
		- Average P(y<1>, y<2>, y<3>)
			<br> = P(y<1>) P(y<2>|y<1>)
			<br> = P(y<3>|y<1>,y<2>)
- Video: Sampling novel sequences
- Video: Vanishing gradient with RNNs
- Video: Gated Recurrent Unit (GRU)
- Video: Long Short Term Memory (LSTM)
- Video: Bidirectional RNN
- Video: Deep RNNs
- Quiz: Recurrent Neural Networks
- Programming Assignment: Building a recurrent neural network - step by step
- Programming Assignment: Dinosaur Island - Character-Level Language Modeling
- Programming Assignment: Jazz Improvisation with LSTM

### 2주차 (Week 2)
#### Natural Language Processing & Word Embeddings
- Video: Word Representation
	- Word Representation
		- 1-hot representation
			- Ex) I want a glass of orange juice.
			- Ex) I want a glass of apple juice.
	- Featurized representation: word embedding
	- Visualizing word embeddings
		- 300D → 2D (t-SNE)
- Video: Using Word Embeddings
	- Named entity recognition example
		- [Sally] [Johnson] [is] [an] [orange] [farmer]
		- [Robert] [Lin] [is] [an] [apple] [farmer]
	- Transfer learning and word embeddings
		- Learn word embeddings from large text corpus. (1-100B words)<br>(Or download pre-trained embedding online.)
		- Transfer embedding to new task with smaller training set.<br>(Say, 100k words)
		- Optional: Continue to finetune the word embeddings with new data.
	- Relation to face encoding
- Video: Properties of word embeddings
	- Analogies
		- Man → Woman is King → Queen?
	- Analogies using word vectors
		- e_man - e_woman ≒ e_king - e_?
		- Find word w: argmax_w sim(e_w, e_king - e_man + e_woman)
			- e_man - e_woman ≒ e_king - e_w
			- e_king - e_man + e_woman: 30\~75%
	- Cosine Similarity
		- Sim(e_w, e_king - e_man + e_woman)
			- Sim(u, v) = (u.T * v) / (L2norm.u * L2norm.v)
			- Ex) Man : Woman as Boy : Girl
			- Ex2) Ottawa : Canada as Nairobi : Kenya
			- Ex3) Big : Bigger as Tall : Taller
			- Ex4) Yen : Japan as Ruble : Russia
- Video: Embedding matrix
	- Embedding matrix
		- In practice, use specialized function to look up an embedding.
- Video: Learning word embeddings
	- Neural language model
	- Other context/target pairs
		- Ex) I want a glass of orange juice to go along with my cereal.
		- A glass of orange: context
			- Last 4 words
			- 4 words on left & right
			- Last 1 word
			- Nearby 1 word
			- Skipgram
		- Juice: target
- Video: Word2Vec
	- Skipgrams
		- Ex) I want a glass of orange juice to go along with my cereal.
			- Context
				- orange
			- Target
				- Juice
				- Glass
				- My
				- …
	- Model
		- Vocab size = 10,000k
		- Context c ("orange") [6257] → Target t ("juice") [4834]
			- X → Y
		- O_c → E → e_c → O(softmax) → y_hat
	- Problems with softmax classification
		- Hierarchical softmax
		- How to xample the context c?
			- The, of, a, and, to, …
			- Orange, apple, durian, ... 
- Video: Negative Sampling
	- Defining a new learning problem
		- Ex) I want a glass of orange juice to go along with my cereal.
		- Context	Word	Target
		Orange	Juice	1
		Orange	King	0
		Orange	Book	0
		Orange	The	0
		Orange	Of	0
		- K = 5\~20 for smaller datasets
		- K = 2\~5 for large dataset
	- Model
	- Selecting negative examples
- Video: GloVe word vectors
	- Ex) I want a glass of orange juice to go along with my cereal.
		- c, t
		- X_ij = # times i (=t) appears in context of j (=c).
		- X_ij = X_ji
	- Model
	- A note on the featurization view of word embeddings
- Video: Sentiment Classifications
	- Sentiment classification problem
		- Example (X → Y)
			- The dessert is excellent. ★★★★☆
			- Service was quite low. ★★☆☆☆
			- Good for a quick meal, but nothing special. ★★★☆☆
			- Completely lacking in good taste, good service, and good ambience. ★☆☆☆☆
			- 10,000 → 100,000 words
	- Simple sentiment classification model
		- The dessert is excellent. ★★★★☆
			- [8928] [2468] [4694] [3180]
			- The
				- o_8928 → E → e_8928
			- Desert
				- o_2468 → E → e_2468
			- Is
				- o_4694 → E → e_4694
			- Excellent
				- o_3180 → E → e_3180
			- Average all up (300D) → O(softmax) (1\~5) → y_hat
			- This model will have bad result with following sentence:
				- Completely lacking in good taste, good service, and good ambience.
				- Because the sentence contains 3 'good' words, it will have positive prediction.
	- RNN for sentiment classification
		- Many-to-One
		- "not good"
- Video: Debiasing word embeddings
	- The problem of bias in word embeddings
		- Man : Woman as King : Queen
		- Man : Computer_Programmer as Woman : Homemaker (X)
		- Father : Doctor as Mother : Nurse (X)
		- Word embeddings can reflect gender, ethnicity, age, sexual orientation, and other biases of the text used to train the model.
	- Addressing bias in word embeddings
		- Identify bias direction.
			- e_he - e_she
			- e_male - e_female
			- …
			- Average all up
		- Neutralize: For every word that is not definitional, project to get rid of bias.
		- Equalize pairs.
- Quiz: Natural Language Processing & Word Embeddings
- Programming Assignment: Operations on Word Vectors - Debiasing
- Programming Assignment: Emojify

### 3주차 (Week 3)
#### Sequence Models & Attention Mechanism
- Video: Basic Models
- Video: Picking the most likely sentence
- Video: Beam Search
- Video: Refinements to Beam Search
- Video: Error Analysis in Beam Search
- Video: Blue Score (optional)
- Video: Attention Model Intuition
- Video: Attention Model
- Video: Speech Recognition
- Video: Trigger Word Detection
- Video: Conclusion and Thank You
- Quiz: Sequence Models & Attention Mechanism
- Programming Assignment: Neural Machine Translation with Attention
- Programming Assignment: Trigger Word Detection